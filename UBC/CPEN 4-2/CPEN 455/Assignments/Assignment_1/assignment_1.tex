\documentclass{article}
\usepackage[utf8]{inputenc}
% \usepackage[paperheight=16cm, paperwidth=12cm,% Set the height and width of the paper
% includehead,
% nomarginpar,% We don't want any margin paragraphs
% textwidth=10cm,% Set \textwidth to 10cm
% headheight=10mm,% Set \headheight to 10mm
% ]{geometry}
\usepackage{geometry}
\usepackage{fancyhdr}
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{xspace}
\usepackage{bm}
\usepackage{xcolor}
\usepackage[colorlinks,linkcolor=blue]{hyperref}
\usepackage{graphicx}
\newenvironment{solution}
  {\par\noindent\textbf{Solution:}\par}
  {\par}

\makeatletter
\DeclareRobustCommand\onedot{\futurelet\@let@token\@onedot}
\def\@onedot{\ifx\@let@token.\else.\null\fi\xspace}
\def\iid{\emph{i.i.d}\onedot} \def\IID{\emph{I.I.D}\onedot}
\def\eg{\emph{e.g}\onedot} \def\Eg{\emph{E.g}\onedot}
\def\ie{\emph{i.e}\onedot} \def\Ie{\emph{I.e}\onedot}
\def\cf{\emph{c.f}\onedot} \def\Cf{\emph{C.f}\onedot}
\def\etc{\emph{etc}\onedot} \def\vs{\emph{vs}\onedot}
\def\wrt{w.r.t\onedot} \def\dof{d.o.f\onedot}
\def\aka{\emph{a.k.a}\onedot}
\def\etal{\emph{et al}\onedot}
\makeatother

\newcommand{\important}[1]{{\color{blue}{\bf\sf #1}}}

\title{CPEN455: Deep Learning \\ Homework 1}
\author{Mercury Mcindoe 85594505}
\date{January 19th 2025}

\begin{document}

\pagestyle{fancy}
\fancyhead{} % clear all header fields
\fancyhead[L]{\textbf{UBC CPEN455 2024 Winter Term 2 }}
\fancyhead[R]{\textbf{Homework 1}}

\maketitle
\thispagestyle{fancy}


\section{Problem 1}
\subsection{}
\begin{solution}

  We need rescale by $\frac{1}{1-p}$ to ensure that the expected activation values maintain remain unchaged between training and inference.

\end{solution}
\subsection{}
\begin{solution}
  Let's first consider the case before Dropout ($i.e., \textbf{h}$). Since $\textbf{x} \sim \mathcal{N}(\textbf{0}, I)$, 
  each entry $x_i$ within $\textbf{x}$ follows a normal distribution $\mathcal{N}(0,1)$ and each are iid.
  Let $\textbf{z} = W\textbf{x}$,
  $$\mathbb{E}[\textbf{z}] = \mathbb{E}[W\textbf{x}] = W\mathbb{E}[\textbf{x}] = 0$$
  $$\text{Var}(\textbf{z}) = \text{Var}(W\textbf{x}) = W\text{Var}(\textbf{x})W^T = W
  \begin{bmatrix} 
    \text{Var}(x_1) & \text{Cov}(x_1,x_2) & \cdots & \text{Cov}(x_1,x_N) \\ 
    \text{Cov}(x_2,x_1) & \text{Var}(x_2) & \cdots & \text{Cov}(x_2,x_N) \\ 
    \vdots & \vdots & \ddots & \vdots \\ 
    \text{Cov}(x_n,x_1) & \text{Cov}(x_n,x_2) & \cdots & \text{Var}(x_n,x_n) 
  \end{bmatrix}W^T$$

  Since we know that each $x_i$ is iid which follows $\mathcal{N}(0,1)$, the variance-covariance matrix of $\textbf{z}$ is then,
  $$\text{Var}(\textbf{z}) = W
  \begin{bmatrix}
    1 & 0 & \cdots & 0 \\ 
    0 & 1 & \cdots & 0 \\ 
    \vdots & \vdots & \ddots & \vdots \\ 
    0 & 0 & \cdots & 1
  \end{bmatrix}
  W^T = WW^T = I$$

  Which shows that $\textbf{z} = W\textbf{x} \sim \mathcal{N}(0,I)$. Now considering that $\sigma(\textbf{z}) = \text{max}(\textbf{z},0)$, each entry $h_i$ would have the following expectations and variances,
  $$\mathbb{E}[h_i] = \int_0^\infty \frac{z}{\sqrt{2\pi}}e^{-\frac{z^2}{2}}dz = \left[ -\frac{1}{\sqrt{2\pi}}e^{-\frac{z^2}{2}} \right]_0^\infty = \frac{1}{\sqrt{2\pi}}$$
  $$E[h_i^2] = \int_0^\infty \frac{z^2}{\sqrt{2\pi}}e^{-\frac{z^2}{2}}dz = \frac{1}{2}$$
  $$\text{Var}(h_i) = \mathbb{E}[h_i^2] - (\mathbb{E}[h_i])^2 = \frac{1}{2} - \frac{1}{2\pi}$$
  
  Putting all these together,
  $$\therefore \text{Var}(\textbf{h}) = \left(\frac{1}{2}-\frac{1}{2\pi}\right)I_M$$


  Now let's consider after Dropout, we know that $\tilde{\textbf{h}} = \frac{\textbf{m}}{1-p} \odot \textbf{h}$, for an try $\tilde{h}_i$,


\begin{align*}
\mathbb{E}[\tilde{h}_i] 
&= \mathbb{E}\left[\frac{m_i}{1-p} \cdot h_i\right] \\
&= \frac{1}{1-p} \mathbb{E}[m_i] \cdot \mathbb{E}[h_i] \\
&= \frac{1-p}{1-p} \mathbb{E}[h_i] \\
&= \frac{1}{\sqrt{2\pi}}.
\end{align*}

The variance of \(\tilde{h}_i\) is:
\begin{align*}
\text{Var}(\tilde{h}_i) 
&= \mathbb{E}[\tilde{h}_i^2] - \mathbb{E}[\tilde{h}_i]^2 \\
&= \mathbb{E}\left[\left(\frac{1}{1-p}\right)^2 \cdot m_i^2 \cdot h_i^2\right] 
   - \left(\mathbb{E}\left[\frac{1}{1-p} \cdot m_i \cdot h_i\right]\right)^2 \\
&= \left(\frac{1}{1-p}\right)^2 \left(\mathbb{E}[m_i^2] \mathbb{E}[h_i^2] 
   - \mathbb{E}[m_i]^2 \cdot \mathbb{E}[h_i]^2\right) \\
&= \frac{1}{1-p} \cdot \mathbb{E}[h_i^2] - \mathbb{E}[h_i]^2 \\
&= \frac{1}{1-p} \cdot \frac{1}{2} - \frac{1}{2\pi}.
\end{align*}
  Hence,
  $$\therefore \text{Var}(\tilde{\textbf{h}}) = \left(\frac{1}{1-p} \cdot \frac{1}{2} - \frac{1}{2\pi}\right)I_M$$ 

\end{solution}


\subsection{}
\begin{solution}
For one unit, the expectation that it is kept is $ 1- p$, then given $M$ units the expectation would be $M \cdot (1-p)$ units kept.
For each unit, we have a probability $1-p$ that it is kept, so if we compute the probabilty that $k$ units are kept, $P(\text{kept} =k)$,
$$P(\text{kept} =k) = {M\choose k} \cdot (1-p)^k\cdot p^{M-k}$$
hence, a binomial distribution with probability $1-p$.
\end{solution}



\subsection{}
\begin{solution}
  First let $M(1-p) = \alpha$, in other words $p = 1- \frac{\alpha}{M}$,
  $$ \lim_{M \to \infty} {M \choose k}\cdot (1-p)^k \cdot p^{M-k} = \lim_{M \to \infty} \frac{M(M-1)\cdot(M-k+1)}{k!}(1-p)^k (1-\frac{\alpha}{M})^{M-k} $$
  $$ = \lim_{M \to \infty} \frac{\left(M\cdot(1-p)\right)\cdot \left((M-1)\cdot(1-p)\right) \cdots \left((M-k+1)(1-p)\right)}{k!} \cdot (1-\frac{\alpha}{M})^{M-k}$$
  $$ = \lim_{M \to \infty} \frac{\left(M\cdot(1-p)\right)\cdot \left((M-1)\cdot(1-p)\right) \cdots \left((M-k+1)(1-p)\right)}{k!} \cdot (1-\frac{\alpha}{M})^{\frac{M}{\alpha}} \cdot (1-\frac{\alpha}{M})^{-k}$$
  $$=\frac{\alpha^k}{k!}e^{-\alpha} = \frac{\left(M(1-p)\right)^k}{k!}e^{-M(1-p)}$$
It becomes a Poisson distribution with parameter $M(1-p)$.
\end{solution}

\subsection{}
\begin{solution}
Let's say that we want to keep $x$ units and get the probability distribution. We then want to sum all the probabilities
of keeping $x$ units for all $M$. Thus we want,
$$P(x \text{ units kept}) = \sum_{M=x}^\infty P(x \text{ units kept} \cap M \text{ units})$$

Let's do the math!
\begin{align*}
P(x \text{ units kept} \cap M \text{ units}) 
&= \frac{\lambda^M e^{-\lambda}}{M!} \cdot {M \choose x} \cdot (1-p)^x \cdot p^{M-x} \\
&= \frac{\lambda^M e^{-\lambda}}{M!} \cdot \frac{M!}{(M-x)! \cdot x!} \cdot (1-p)^x \cdot p^{M-x} \\
&= \frac{e^{-\lambda}}{x!} \cdot (1-p)^x \cdot \frac{\lambda^M \cdot p^{M-x}}{(M-x)!}.
\end{align*}

Now, the probability of \(x\) units being kept is:
\begin{align*}
P(x \text{ units kept}) 
&= \sum_{M=x}^\infty \frac{e^{-\lambda}}{x!} \cdot (1-p)^x \cdot \frac{\lambda^M \cdot p^{M-x}}{(M-x)!} \\
&= \frac{e^{-\lambda}}{x!} \cdot (1-p)^x \cdot \sum_{M=x}^\infty \frac{\lambda^M \cdot p^{M-x}}{(M-x)!}.
\end{align*}

Let \(M' = M - x\). Then:
\begin{align*}
P(x \text{ units kept}) 
&= \frac{e^{-\lambda}}{x!} \cdot (1-p)^x \cdot \sum_{M'=0}^\infty \frac{\lambda^{M'+x} \cdot p^{M'}}{M'!} \\
&= \frac{e^{-\lambda}}{x!} \cdot (1-p)^x \cdot \lambda^x \cdot \sum_{M'=0}^\infty \frac{(\lambda p)^{M'}}{M'!}.
\end{align*}

Since \(\sum_{M'=0}^\infty \frac{(\lambda p)^{M'}}{M'!} = e^{\lambda p}\), we get:
\begin{align*}
P(x \text{ units kept}) 
&= \frac{e^{-\lambda}}{x!} \cdot (1-p)^x \cdot \lambda^x \cdot e^{\lambda p} \\
&= \frac{e^{-\lambda(1-p)} \cdot \{\lambda(1-p)\}^x}{x!}.
\end{align*}

Therefore, the number of kept units follows a Poisson distribution with parameter \(\lambda(1-p)\).
\end{solution}


\section{Problem 2}
\subsection{}
\begin{solution}
We need the hyperparameter $\epsilon$ to avoid division by 0 during training.
\end{solution}

\subsection{}
\begin{solution}
  \[
    \begin{aligned}
\mathbb{E}[\hat{Y}[i,j]] &= \mathbb{E}\left[ \gamma[j] \cdot \frac{Y[i,j] -\mathbf{m}[j]}{\sqrt{\mathbf{v}[j]}} +\boldsymbol{\beta}[j]\right] \\ 
                         &= \frac{\gamma[j]}{\textbf{v}[j]} \cdot \left(\mathbb{E} \left[ Y[i,j] \right]  - \mathbb{E}[\textbf{m}[j]] \right) + \mathbb{E}[\boldsymbol{\beta}[j]]  \\
                         &=  \frac{\gamma[j]}{\textbf{v}[j]} \cdot \left( \textbf{m}[j] - \textbf{m}[j] \right) + \boldsymbol{\beta}[j] \\ 
                         &= \boldsymbol{\beta}[j]
  \end{aligned}
\]

\[
  \begin{aligned}
    Var(\hat{Y}[i,j]) &= \mathbb{E}[\hat{Y}[i,j]^2] - \mathbb{E}[\hat{Y}[i,j]]^2 = \mathbb{E}[\hat{Y}[i,j]^2] - \boldsymbol{\beta}[j]^2\\ 
                      &= \mathbb{E}\left[ \frac{\gamma[j]^2}{\textbf{v}[j]} \cdot (Y[i,j] - \textbf{m}[j])^2 + 2\cdot \frac{\gamma[j] \boldsymbol{\beta}[j]}{\sqrt{\textbf{v}[j]}} \cdot \left( E\left[Y[i,j]  - \textbf{m}[j]\right] \right)  + \boldsymbol{\beta}[j]^2\right]- \boldsymbol{\beta}[j]^2 \\ 
                      &= \frac{\gamma[j]^2}{\textbf{v}[j]} \cdot \mathbb{E}\left[ \left( Y(i,j) - \textbf{m}[j]\right)^2 \right] + 0 + 0 \\ 
                      &= \frac{\gamma[j]^2}{\textbf{v}[j]}\cdot \mathbb{E}\left[ Y[i,j]^2 -2Y[i,j]\textbf{m}[j] + \textbf{m}[j]^2 \right] \\ 
                      &= \frac{\gamma[j]^2}{\textbf{v}[j]}\cdot \left( \mathbb{E}\left[ Y[i,j]^2\right] - 2\textbf{m}[j]^2 + \textbf{m}[j]^2 \right) \\ 
                      &= \frac{\gamma[j]^2}{\textbf{v}[j]}\cdot \left( \textbf{v}[j] - \textbf{m}[j]^2 + \textbf{m}[j]^2  \right)  
          \\            &= \gamma[j]^2
  \end{aligned}
\]
  Thus,
  $$\mathbb{E}\left[ \hat{Y}[i,j] \right]= \boldsymbol{\beta}[j] \quad Var(\hat{Y}[i,j]) = \gamma[j]^2$$
\end{solution}


\subsection{}
\begin{solution}
  Firstly, let's denote the derivative of the ReLU activation function, we denote it as $f$ where $f$ denotes the step function from 0 to 1 where the scalar input equals zero. 
  Also, we will denote the indexes with subscripts, for instance, $Y[i,j]=Y_{ij}$ in the following explanations.

  \textbf{Problem 1:} $\frac{\partial \ell}{\partial \gamma}$
  $$\frac{\partial \ell}{\partial \gamma} = \begin{bmatrix}  \frac{\partial \ell}{\partial \gamma_1} \\ \frac{\partial \ell}{\partial \gamma_2} \\ \vdots \\ \frac{\partial \ell}{\partial \gamma_M}   \end{bmatrix}$$

  Now let's compute $\frac{\partial \ell}{\partial \gamma_j} $ for $ j = 1, \dots, M$.
  \[ 
  \begin{aligned}
    \frac{\partial \ell}{\partial \gamma_j} &= \sum_{k = 1}^{B} \sum_{l = 1}^{M} \frac{\partial \ell}{\partial H_{kl}}\frac{\partial H_{kl}}{\partial \hat{Y}_{kl}}\frac{\partial \hat{Y}_{kl}}{\partial \gamma_j}  \\
                                         \\ &=\sum_{k = 1}^{B} \sum_{l = 1}^{M} \frac{\partial \ell}{\partial H_{kl}}f(\hat{Y}_{kl})\frac{\partial \hat{Y}_{kl}}{\partial \gamma_j}
  \end{aligned}
  \]

  Now let's compute $\frac{\partial \hat{Y}_{kl}}{\partial \gamma{j}}$,

  \[ 
  \begin{aligned} 
    \frac{\partial \hat{Y}_{kl}}{\partial \gamma_j} &= \frac{\partial}{\partial \gamma_j} \left( \gamma_l \cdot \frac{ Y_{kl} - \textbf{m}_l }{\sqrt{\textbf{v}_l + \epsilon}} + \boldsymbol{\beta}_j \right) \\ 
                                                     &=  \delta_{lj} \cdot \frac{ Y_{kl} - \textbf{m}_l }{\sqrt{\textbf{v}_l + \epsilon}}
  \end{aligned}
  \]
  In summary,
  \[
    \begin{aligned}
      \frac{\partial \ell}{\partial \gamma_j} &=\sum_{k = 1}^{B} \sum_{l = 1}^{M} \frac{\partial \ell}{\partial H_{kl}}f(\hat{Y}_{kl})\frac{\partial \hat{Y}_{kl}}{\partial \gamma_j} \\ 
                                              &=\sum_{k=1}^B \frac{\partial \ell}{\partial H_{kj}} f(\hat{Y}_{kj})\frac{Y_{kj} - \textbf{m}_j}{\sqrt{\textbf{v}_j + \epsilon}} 
    \end{aligned}
  \]


  \textbf{Problem 2:} $\frac{\partial l}{\partial \beta}$
  $$ \frac{\partial l}{\partial \beta} = \begin{bmatrix}  \frac{\partial l}{\partial \beta_1} \\  \frac{\partial l}{\partial \beta_2} \\ \vdots \\ \frac{\partial l}{\partial \beta_M}  \end{bmatrix} $$

  Similarly, let's compute $ \frac{\partial l}{\partial \beta_j}$ for $j = 1,\cdots, M$. 
 \[ 
  \begin{aligned}
    \frac{\partial \ell}{\partial \beta_j} &= \sum_{k = 1}^{B} \sum_{l = 1}^{M} \frac{\partial \ell}{\partial H_{kl}}\frac{\partial H_{kl}}{\partial \hat{Y}_{kl}}\frac{\partial \hat{Y}_{kl}}{\partial \beta_j}  \\
                                         \\ &=\sum_{k = 1}^{B} \sum_{l = 1}^{M} \frac{\partial \ell}{\partial H_{kl}}f(\hat{Y}_{kl})\frac{\partial \hat{Y}_{kl}}{\partial \beta_j}
  \end{aligned}
  \]

  Now we compute $\frac{\partial \hat{Y}_{kl}}{\partial \beta_j}$,
  \[
    \begin{aligned}
      \frac{\partial \hat{Y}_{kl}}{\partial \beta_j} &= \frac{\partial}{\partial \beta_j}  \left( \gamma_l \cdot \frac{ Y_{kl} - \textbf{m}_l }{\sqrt{\textbf{v}_l + \epsilon}} + \beta_l \right) \\  
                        &= \delta_{lj}
    \end{aligned}
  \]

  In summary,
  \[
      \begin{aligned}
      \frac{\partial \ell}{\partial \beta_j} &=\sum_{k = 1}^{B} \sum_{l = 1}^{M} \frac{\partial \ell}{\partial H_{kl}}f(\hat{Y}_{kl})\frac{\partial \hat{Y}_{kl}}{\partial \beta_j} \\ 
                                             &=\sum_{k=1}^B \frac{\partial \ell}{\partial H_{kj}} f(\hat{Y}_{kj})
      \end{aligned}
  \]

  \textbf{Problem 3:}$\frac{\partial \ell}{\partial \textbf{m}}$
  $$\frac{\partial \ell}{\partial \textbf{m}} = \begin{bmatrix} \frac{\partial \ell}{\partial \textbf{m}_1} \\ \frac{\partial \ell}{\partial \textbf{m}_2}  \\ \vdots \\ \frac{\partial \ell}{\partial \textbf{m}_M}  \end{bmatrix}$$

  Let's find $\frac{\partial \ell}{\partial \textbf{m}_j}$ for $j = 1,\dots,M$.
\[ 
  \begin{aligned}
    \frac{\partial \ell}{\partial \textbf{m}_j} &= \sum_{k = 1}^{B} \sum_{l = 1}^{M} \frac{\partial \ell}{\partial H_{kl}}\frac{\partial H_{kl}}{\partial \hat{Y}_{kl}}\frac{\partial \hat{Y}_{kl}}{\partial \textbf{m}_j}  \\
                                             \\ &=\sum_{k = 1}^{B} \sum_{l = 1}^{M} \frac{\partial \ell}{\partial H_{kl}}f(\hat{Y}_{kl})\frac{\partial \hat{Y}_{kl}}{\partial \textbf{m}_j}
  \end{aligned}
  \]
  If we compute $\frac{\partial \hat{Y}_{kl}}{\partial \textbf{m}_j}$, 
\[
    \begin{aligned}
      \frac{\partial \hat{Y}_{kl}}{\partial \textbf{m}_j} &= \frac{\partial}{\partial \textbf{m}_j}  \left( \gamma_l \cdot \frac{ Y_{kl} - \textbf{m}_l }{\sqrt{\textbf{v}_l + \epsilon}} + \beta_l \right) \\  
                                                          &= - \delta_{lj}\cdot \gamma_l \cdot \frac{1}{\sqrt{\textbf{v}_l + \epsilon}} + \gamma_l \cdot (Y_{kl} - \textbf{m}_l) \cdot (-\frac{1}{2}) \cdot (\textbf{v}_l + \epsilon)^{-\frac{3}{2}} \cdot \frac{\partial \textbf{v}_l}{\partial \textbf{m}_j}  
    \end{aligned}
  \]

  Let's investigate what $\frac{\partial \textbf{v}_l}{\partial \textbf{m}_j}$ is,
  \[
  \begin{aligned}
    \frac{\partial \textbf{v}_l}{\partial \textbf{m}_j} &= \frac{\partial}{\partial \textbf{m}_j} \left( \frac{1}{B}\sum_{i=1}^B (Y_{il} - \textbf{m}_l)^2 \right)  \\ 
                                                        &=\frac{2}{B} \sum_{i=1}^B (Y_{il} - \textbf{m}_l) \cdot \frac{\partial \textbf{m}_l}{\partial \textbf{m}_j} \\ 
                                                        &= \delta_{lj}\cdot \frac{2}{B} \sum_{i=1}^B (Y_{il} - \textbf{m}_l) \\ 
                                                        &= 0
  \end{aligned}
  \]

  Hence,
    \[
      \begin{aligned}
        \frac{\partial \ell}{\partial \textbf{m}_j} &=\sum_{k = 1}^{B} \sum_{l = 1}^{M} \frac{\partial \ell}{\partial H_{kl}}f(\hat{Y}_{kl})\frac{\partial \hat{Y}_{kl}}{\partial \textbf{m}_j} \\ 
                                                    &= \sum_{k = 1}^{B} \sum_{l = 1}^{M} \frac{\partial \ell}{\partial H_{kl}}f(\hat{Y}_{kl})\left(  - \delta_{lj}\cdot \gamma_l \cdot \frac{1}{\sqrt{\textbf{v}_l + \epsilon}} \right)  \\ 
                                                    &=\sum_{k=1}^B \frac{\partial \ell}{\partial H_{kj}} f(\hat{Y}_{kj}) \left( -\gamma_j \cdot \frac{1}{\sqrt{\textbf{v}_j + \epsilon}} \right)
      \end{aligned}
  \]

  \textbf{Problem 4:}$\frac{\partial \ell}{\partial \textbf{v}}$
  $$\frac{\partial \ell}{\partial \textbf{v}} = \begin{bmatrix} \frac{\partial\ell}{\partial\textbf{v}_1} \\ \frac{\partial\ell}{\partial\textbf{v}_2} \\ \vdots \\ \frac{\partial\ell}{\partial\textbf{v}_M} \end{bmatrix}$$

  As usual, let's compute $\frac{\partial\ell}{\partial\textbf{v}_j}$ for $j =1,\dots,M$.
\[ 
  \begin{aligned}
    \frac{\partial \ell}{\partial \textbf{v}_j} &= \sum_{k = 1}^{B} \sum_{l = 1}^{M} \frac{\partial \ell}{\partial H_{kl}}\frac{\partial H_{kl}}{\partial \hat{Y}_{kl}}\frac{\partial \hat{Y}_{kl}}{\partial \textbf{v}_j}  \\
                                             \\ &=\sum_{k = 1}^{B} \sum_{l = 1}^{M} \frac{\partial \ell}{\partial H_{kl}}f(\hat{Y}_{kl})\frac{\partial \hat{Y}_{kl}}{\partial \textbf{v}_j}
  \end{aligned}
  \]
 Let's compute $\frac{\partial \hat{Y}_{kl}}{\partial \textbf{v}_j}$,
  \[
    \begin{aligned}
       \frac{\partial \hat{Y}_{kl}}{\partial \textbf{v}_j} &= \frac{\partial}{\partial \textbf{v}_j}  \left( \gamma_l \cdot \frac{ Y_{kl} - \textbf{m}_l }{\sqrt{\textbf{v}_l + \epsilon}} + \beta_l \right) \\  
                                                           &= \gamma_l \cdot (Y_{kl}- \textbf{m}_l) \cdot (-\frac{1}{2}) \cdot (\textbf{v}_l + \epsilon)^{-\frac{3}{2}}\cdot \frac{\partial \textbf{v}_l}{\partial \textbf{v}_j} \\ 
                                                           &= \delta_{lj} \cdot \gamma_l \cdot (Y_{kl} - \textbf{m}_l) \cdot (-\frac{1}{2}) \cdot (\textbf{v}_l + \epsilon)^{-\frac{3}{2}}
     \end{aligned}
  \]
  Therefore,
      \[
      \begin{aligned}
        \frac{\partial \ell}{\partial \textbf{v}_j} &=\sum_{k = 1}^{B} \sum_{l = 1}^{M} \frac{\partial \ell}{\partial H_{kl}}f(\hat{Y}_{kl})\frac{\partial \hat{Y}_{kl}}{\partial \textbf{v}_j} \\ 
                                                    &= \sum_{k = 1}^{B} \sum_{l = 1}^{M} \frac{\partial \ell}{\partial H_{kl}}f(\hat{Y}_{kl})\left( \delta_{lj} \cdot \gamma_l \cdot (Y_{kl} - \textbf{m}_l) \cdot (-\frac{1}{2}) \cdot (\textbf{v}_l + \epsilon)^{-\frac{3}{2}}
 \right)  \\ 
                                                    &=\sum_{k=1}^B \frac{\partial \ell}{\partial H_{kj}} f(\hat{Y}_{kj}) \left( -\gamma_j \cdot \frac{1}{2} \cdot (Y_{kj} -\textbf{m}_j) \cdot (\textbf{v}_j + \epsilon)^{-\frac{3}{2}}  \right)
      \end{aligned}
  \]
  \textbf{Problem 5:}$\frac{\partial \ell}{\partial Y}$
  $$\frac{\partial \ell}{\partial Y} = \begin{bmatrix} 
    \frac{\partial \ell}{\partial Y_{11}} & \cdots & \frac{\partial \ell}{\partial Y_{1M}} \\ 
    \vdots & \frac{\partial \ell}{\partial Y_{st}} & \vdots \\ 
    \frac{\partial \ell}{\partial Y_{B1}} & \cdots & \frac{\partial \ell}{\partial Y_{BM}}
  \end{bmatrix}$$

  Now, we compute $\frac{\partial \ell}{\partial Y_{st}}$ for $s=1,\dots,B$ and $t=1,\dots,M$.
  \[
    \begin{aligned}
      \frac{\partial \ell}{\partial Y_{st}} &= \sum_{k=1}^B\sum_{l=1}^M  \frac{\partial \ell}{\partial H_{kl}}f(\hat{Y}_{kl})\frac{\partial \hat{Y}_{kl}}{\partial Y_{st}} 
    \end{aligned}
  \]

  Let's compute $\frac{\partial \hat{Y}_{kl}}{\partial Y_{st}}$,
  \[
    \begin{aligned}
      \frac{\partial \hat{Y}_{kl}}{\partial Y_{st}} &= \frac{\partial}{\partial Y_{st}}  \left( \gamma_l \cdot \frac{ Y_{kl} - \textbf{m}_l }{\sqrt{\textbf{v}_l + \epsilon}} + \beta_l \right) \\ 
                                                    &= \gamma_l \cdot (\frac{\partial Y_{kl}}{\partial Y_{st}} - \frac{\partial \textbf{m}_l}{\partial Y_{st}})\cdot \frac{1}{\sqrt{\textbf{v}_l + \epsilon}} -\frac{1}{2} \cdot \gamma_l \cdot (Y_{kl} -\textbf{m}_l)\cdot (\textbf{v}_l + \epsilon)^{-\frac{3}{2}}\cdot \frac{\partial \textbf{v}_l}{\partial Y_{st}}
     \end{aligned}
  \]
  We know have to find out $\frac{\partial Y_{kl}}{\partial Y_{st}}, \frac{\partial \textbf{m}_l}{\partial Y_{st}},\frac{\partial \textbf{v}_l}{\partial Y_{st}}$,
  $$\frac{\partial Y_{kl}}{\partial Y_{st}} = \delta_{ks}\cdot \delta_{lt}$$

  \[
    \begin{aligned}
      \frac{\partial \textbf{m}_l}{\partial Y_{st}} &= \frac{\partial}{\partial Y_{st}} \left( \frac{1}{B}\sum_{i=1}^B Y_{il} \right) \\ 
                                                    &= \frac{1}{B} \cdot \delta_{lt} 
    \end{aligned}
  \]

  \[
    \begin{aligned}
      \frac{\partial \textbf{v}_l}{\partial Y_{st}} &= \frac{\partial}{\partial Y_{st}} \left(\frac{1}{B} \sum_{i=1}^B (Y_{il} - \textbf{m}_l)^2\right) \\ 
                                                    &= \frac{2}{B} \sum_{i=1}^B (Y_{il} - \textbf{m}_l)\cdot\left(\frac{\partial Y_{il}}{\partial Y_{st}} - \frac{\partial \textbf{m}_l}{\partial Y_{st}}\right) \\ 
                                                    &= \frac{2}{B} \sum_{i=1}^B (Y_{il} - \textbf{m}_l) \cdot (\delta_{is}\cdot \delta_{lt} -\frac{1}{B} \cdot \delta_{lt}) \\ 
                                                    &= \frac{2}{B} \cdot \delta_{lt} \cdot (Y_{sl} - \textbf{m}_l) -\frac{2}{B^2} \cdot \delta_{lt} \cdot \sum_{i=1}^B (Y_{il} - \textbf{m}_l) \\ 
                                                    &= \frac{2}{B} \cdot \delta_{lt} \cdot (Y_{sl} - \textbf{m}_l)
    \end{aligned}
  \]

  Putting it all together,
  \[ 
    \begin{aligned}
      \frac{\partial \hat{Y}_{kl}}{\partial Y_{st}} &=  \gamma_l \cdot (\delta_{ks}\cdot \delta_{lt}- \frac{1}{B} \cdot \delta_{lt})\cdot \frac{1}{\sqrt{\textbf{v}_l + \epsilon}} -\frac{1}{2} \cdot \gamma_l \cdot (Y_{kl} -\textbf{m}_l)\cdot (\textbf{v}_l + \epsilon)^{-\frac{3}{2}}\cdot \frac{2}{B} \cdot \delta_{lt} \cdot (Y_{sl} - \textbf{m}_l)
    \end{aligned}
  \]

  And,
  \[
    \begin{aligned}
      \frac{\partial \ell}{\partial Y_{st}} &= \sum_{k=1}^B\sum_{l=1}^M  \frac{\partial \ell}{\partial H_{kl}}f(\hat{Y}_{kl})\frac{\partial \hat{Y}_{kl}}{\partial Y_{st}} \\ 
                                            &=\sum_{k=1}^B\sum_{l=1}^M  \frac{\partial \ell}{\partial H_{kl}}f(\hat{Y}_{kl})\left\{  \gamma_l \cdot (\delta_{ks}\cdot \delta_{lt}- \frac{1}{B} \cdot \delta_{lt})\cdot \frac{1}{\sqrt{\textbf{v}_l + \epsilon}} -\frac{1}{2} \cdot \gamma_l \cdot (Y_{kl} -\textbf{m}_l)\cdot (\textbf{v}_l + \epsilon)^{-\frac{3}{2}}\cdot \frac{2}{B} \cdot \delta_{lt} \cdot (Y_{sl} - \textbf{m}_l)\right\} \\
                                            &=\sum_{k=1}^B\sum_{l=1}^M  \frac{\partial \ell}{\partial H_{kl}}f(\hat{Y}_{kl})\left\{  \gamma_l \cdot (\delta_{ks}\cdot \delta_{lt}- \frac{1}{B} \cdot \delta_{lt})\cdot (\textbf{v}_l + \epsilon)^{-\frac{1}{2}} - \frac{1}{B} \cdot\gamma_l \cdot (Y_{kl} -\textbf{m}_l)\cdot (\textbf{v}_l + \epsilon)^{-\frac{3}{2}}\cdot  \delta_{lt} \cdot (Y_{sl} - \textbf{m}_l)\right\} \\
                                            &= \frac{\partial \ell}{\partial H_{st}}f(\hat{Y}_{st}) \cdot \gamma_{t} \cdot (\textbf{v}_t + \epsilon)^{-\frac{1}{2}} -\frac{1}{B} \sum_{k=1}^B \frac{\partial \ell}{\partial H_{kt}}f(\hat{Y}_{kt})\cdot \gamma_t \cdot \left\{ (\textbf{v}_t + \epsilon)^{-\frac{1}{2}} - (Y_{kt} - \textbf{m}_t)\cdot (\textbf{v}_t + \epsilon)^{-\frac{3}{2}}\cdot (Y_{st} - \textbf{m}_t) \right\} 
    \end{aligned}
  \]

\end{solution}


\section{Problem 3}
\subsection{}
\begin{solution}
  From class, we learned that,
  \[
    \begin{aligned}
      \frac{\partial \ell}{\partial \textbf{h}_L} &=\left( \frac{\partial \textbf{y}}{\partial \textbf{h}_L} \right)^T \frac{\partial \ell}{\partial \textbf{y}} \\ 
      \frac{\partial \ell}{\partial \textbf{h}_{L-1}} &=\left( \frac{\partial \textbf{h}_L}{\partial \textbf{h}_{L-1}} \right)^T \frac{\partial \ell}{\partial \textbf{h}_L} 
    \end{aligned}
  \]

  Then for $1 \le i \le L$, we can say that,
  $$\frac{\partial \ell}{\partial \textbf{h}_i}=\left( \frac{\partial \textbf{h}_{i+1}}{\partial \textbf{h}_{i}} \right)^T \left( \frac{\partial \textbf{h}_{i+2}}{\partial \textbf{h}_{i+1}} \right)^T \cdots \left(\frac{\partial \textbf{y}}{\partial \textbf{h}_L} \right)^T \frac{\partial \ell}{\partial \textbf{y}}$$

  Let's first compute $\frac{\partial \ell}{\partial \textbf{y}}$,
  \[
    \frac{\partial \ell}{\partial \textbf{y}} = \begin{bmatrix} \frac{\partial \ell}{\partial \textbf{y}_1} \\ \vdots \\ \frac{\partial \ell}{\partial \textbf{y}_{D_L}} \end{bmatrix} \in \mathbb{R}^{D_L \times 1} 
  \]

  Then, for $\frac{\partial \ell}{\partial \textbf{y}_j}$, for $j =1,\dots,D_L$.
  \[
    \begin{aligned}
      \frac{\partial \ell}{\partial \textbf{y}_j} &= \frac{\partial}{\partial \textbf{y}_j} \left( - \sum_{k=1}^K \bar{\textbf{y}}[k]\cdot \log(\textbf{y}[k]) \right) \\ 
                                                  &= -\sum_{k=1}^K \bar{\textbf{y}}[k] \cdot \frac{ \frac{\partial \textbf{y}[k]}{\partial \textbf{y}[j]}}{\textbf{y}[k]} \\ 
                                                  &= -\frac{\bar{\textbf{y}}[j]}{\textbf{y}[j]}
    \end{aligned}
  \]

  Now let's find $\frac{\partial \textbf{h}_{k + 1}}{\partial \textbf{h}_{k}}$ for $k=i,\dots,L-1$. We are aware that $\textbf{h}_i = \sigma \left(W_i\textbf{h}_{i-1} + \textbf{b}_i \right)$. \\ 
  If we let $\textbf{z}_i = W_i\textbf{h}_{i-1} +\textbf{b}_i$, then,
  \[
    \begin{aligned}
      \frac{\partial \textbf{h}_{k+1}}{\partial \textbf{h}_k} &= \frac{\partial \textbf{h}_{k+1}}{\partial \textbf{z}_{k+1}}\cdot \frac{\partial \textbf{z}_{k+1}}{\partial \textbf{h}_k}  \\ 
                                                              &= \text{diag}(\sigma'(\textbf{z}_{k+1})) \cdot W_{k+1} \in \mathbb{R}^{D_{k+1} \times D_k}
    \end{aligned}
  \]
  Now, let's get $\frac{\partial \textbf{y}}{\partial \textbf{h}_L}$,
\[
  \frac{\partial \textbf{y}}{\partial \textbf{h}_L} = \begin{bmatrix}     
    \frac{\partial \textbf{y}[1]}{\partial \textbf{h}_L[1]} & \cdots & \frac{\partial \textbf{y}[1]}{\partial \textbf{h}_L[D_L]} \\ 
    \vdots & & \vdots \\ 
    \frac{\partial \textbf{y}[D_L]}{\partial \textbf{h}_L[1]} & \cdots  & \frac{\partial \textbf{y}[D_L]}{\partial \textbf{h}_L[D_L]} 

  \end{bmatrix} \in \mathbb{R}^{D_L \times D_L}
\]
So let's try computing $\frac{\partial \textbf{y}_i}{\partial \textbf{h}_L[k]}$,
\[
  \begin{aligned}
    \frac{\partial \textbf{y}_i}{\partial \textbf{h}_L[k]} &= \frac{\partial}{\partial \textbf{h}_L[k]} \left( \frac{\text{exp}(\textbf{h}_L[i])}{\sum_j \text{exp}(\textbf{h}_L[j])} \right) \\ 
                                                           &= \frac{\partial \textbf{h}_L[i]}{\partial \textbf{h}_L[k]} \cdot \left( \frac{\text{exp}(\textbf{h}_L[i])}{\sum_j \text{exp}(\textbf{h}_L[j])} \right) - \text{exp}(\textbf{h}_L[i])\cdot \frac{\sum_j \frac{\partial \textbf{h}_L[j]}{\partial \textbf{h}_L[k]}\cdot \text{exp}(\textbf{h}_L[j])}{\left( \sum_j \text{exp}(\textbf{h}_L[j]) \right)^2} \\ 
                                                           &= \delta_{ki} \cdot \textbf{y}[i] - \text{exp}(\textbf{h}_L[i]) \cdot \frac{\text{exp}(\textbf{h}_L[k])}{\left( \sum_j \text{exp}(\textbf{h}_L[j]) \right)^2} \\ 
                                                           &= \delta_{ki} \cdot \textbf{y}[i] - \textbf{y}[i] \cdot \textbf{y}[k]
  \end{aligned}
\]

For simplicity, let's pre-compute $\left( \frac{\partial \textbf{y}}{\partial \textbf{h}_L} \right)^T \frac{\partial \ell}{\partial \textbf{y}}$,
\[
  \begin{aligned}
    \left( \frac{\partial \textbf{y}}{\partial \textbf{h}_L} \right)^T \frac{\partial \ell}{\partial \textbf{y}} &= \begin{bmatrix} \textbf{y}[1] - \textbf{y}[1]^2 & -\textbf{y}[2]\textbf{y}[1] & \cdots & -\textbf{y}[D_L]\textbf{y}[1] \\ 
      -\textbf{y}[1]\textbf{y}[2] & \textbf{y}[2]-\textbf{y}[2]^2 & \cdots & -\textbf{y}[D_L]\textbf{y}[2] \\
      \vdots & \vdots & \cdots & \vdots \\ 
       -\textbf{y}[1]\textbf{y}[D_L] & -\textbf{y}[2]\textbf{y}[D_L] & \cdots &  \textbf{y}[D_L] - \textbf{y}[D_L]^2 \\
       \end{bmatrix} \begin{bmatrix} -\frac{\bar{\textbf{y}}[1]}{\textbf{y}[1]} \\ \vdots  \\ -\frac{\bar{\textbf{y}}[D_L]}{\textbf{y}[D_L]}  \end{bmatrix}
  \\
                                     &= \begin{bmatrix} -\bar{\textbf{y}}[1] + \textbf{y}[1]\left( \sum_j \bar{\textbf{y}}[j] \right) \\ 
                                     -\bar{\textbf{y}}[2] + \textbf{y}[2]\left( \sum_j \bar{\textbf{y}}[j] \right) \\ 
                                        \vdots \\ 
                                    -\bar{\textbf{y}}[D_L] + \textbf{y}[D_L]\left( \sum_j \bar{\textbf{y}}[j] \right) 
                                   \end{bmatrix}
                                  \\
                                     &= \begin{bmatrix} \textbf{y}[1] - \bar{\textbf{y}}[1] \\ 
                                      \textbf{y}[2] - \bar{\textbf{y}}[2] \\ 
                                    \vdots \\ 
                                \textbf{y}[D_L] - \bar{\textbf{y}}[D_L] 
                                  \end{bmatrix}
     \end{aligned}
\]

Thus,
\[
  \begin{aligned}
    \frac{\partial \ell}{\partial \textbf{h}_i} &=\left( \frac{\partial \textbf{h}_{i+1}}{\partial \textbf{h}_{i}} \right)^T \left( \frac{\partial \textbf{h}_{i+2}}{\partial \textbf{h}_{i+1}} \right)^T \cdots \left(\frac{\partial \textbf{y}}{\partial \textbf{h}_L} \right)^T \frac{\partial \ell}{\partial \textbf{y}} \\ 
                                                &=      \left( \text{diag}(\sigma'(\textbf{z}_{i+1})) \cdot W_{i+1} \right)^T \left( \text{diag}(\sigma'(\textbf{z}_{i+2})) \cdot W_{i+2} \right)^T \cdots \left( \text{diag}(\sigma'(\textbf{z}_{L})) \cdot W_{L} \right)^T \left(   \begin{bmatrix} \textbf{y}[1] - \bar{\textbf{y}}[1] \\ 
                                      \textbf{y}[2] - \bar{\textbf{y}}[2] \\ 
                                    \vdots \\ 
                                \textbf{y}[D_L] - \bar{\textbf{y}}[D_L] 
                            \end{bmatrix}\right) \in \mathbb{R}^{D_{i} \times 1}
  \end{aligned}
\]
\end{solution}

\subsection{}
\begin{solution}
  Let's start with $\frac{\partial \ell}{\partial W_i} \in \mathbb{R}^{D_{i-1} \times D_{i}}$,
  \[
    \frac{\partial \ell}{\partial W_i} = \begin{bmatrix} 
      \frac{\partial \ell}{\partial W_i[1,1]} & \frac{\partial \ell}{\partial W_i[2,1]} & \cdots & \frac{\partial \ell}{\partial W_i[D_{i},1]} \\ 
      \vdots & \vdots & \cdots & \vdots \\ 
      \frac{\partial \ell}{\partial W_i[1,D_{i-1}]} & \cdots & \cdots & \frac{\partial \ell}{\partial W_i[D_{i},D_{i-1}]}
      
    \end{bmatrix}
  \]
  Using the same setup as the prior question, $\textbf{h}_i = \sigma(W_i\textbf{h}_{i-1} + \textbf{b}_i), \textbf{z}_i = W_i\textbf{h}_{i-1} + \textbf{b}_i$,
  \[
  \begin{aligned}
    \frac{\partial \ell}{\partial W_i[x,y]} &= \frac{\partial \ell}{\partial \textbf{z}_i[x]} \cdot \frac{\partial z_i[x]}{\partial W_i[x,y]} \\ 
                                            &= \frac{\partial \ell}{\partial \textbf{z}_i[x]} \cdot \textbf{h}_{i-1}[y] \\ 
  \end{aligned}
  \]
  Given this, if we were to compute $\frac{\partial \ell}{\partial \textbf{z}_i}$,
  \[
    \begin{aligned}
      \frac{\partial \ell}{\partial \textbf{z}_i} &= \left( \frac{\partial \textbf{h}_i}{\partial  \textbf{z}_i} \right)^T \frac{\partial \ell}{\partial \textbf{h}_i} \\ 
                                         &= \sigma'(\textbf{z}_i) \odot \frac{\partial \ell}{\partial \textbf{h}_i}  
    \end{aligned}
  \]

  These combined will give us,
  $$ \frac{\partial \ell}{\partial W_i} =\textbf{h}_{i-1} \left( \sigma'(\textbf{z}_i) \odot \frac{\partial \ell}{\partial \textbf{h}_i} \right) ^T \in \mathbb{R}^{D_{i-1} \times D_{i}} $$
  where $\frac{\partial \ell}{\partial \textbf{h}_i} \in \mathbb{R}^{D_i \times 1}$. \\ 
  Now, let's compute $\frac{\partial \ell}{\partial \textbf{b}_i}$, 
  \[
    \begin{aligned}
      \frac{\partial \ell}{\partial \textbf{b}_i} &= \left(\frac{\partial \textbf{z}_i}{\partial \textbf{b}_i} \right)^T \left( \frac{\partial \textbf{h}_i}{\partial \textbf{z}_i}\right)^T \frac{\partial \ell}{\partial \textbf{h}_i}  \\ 
                                                  &= I \left( \sigma'(\textbf{z}_i) \odot \frac{\partial \ell}{\partial \textbf{h}_i} \right) \\ 
                                                  &= \sigma'(\textbf{z}_i) \odot \frac{\partial \ell}{\partial \textbf{h}_i} \in \mathbb{R}^{D_i \times 1 }
    \end{aligned}
    \]
\end{solution}

\subsection{}
\begin{solution}
  \textbf{Forward Pass:} \\ 
  Let's start with identifying some terms and the ones given, 
  \[
    \begin{aligned}
      \textbf{h}_i &= \sigma(W_i\textbf{h}_{i-1} +\textbf{b}_i) \\
      \textbf{z}_i &= W_i\textbf{h}_{i-1}+\textbf{b}_i \\
      \textbf{h}_i             &= \sigma(\textbf{z}_i)
    \end{aligned}
  \]


  Now, let's derive the variance $Var(\textbf{z}_i)$, we assume that $\textbf{b}_i = 0$ and the elements of
  $W_i$ are iid and each element has zero mean. Also, the elements $\textbf{h}_{i-1}$ of are iid. We also assume that $\textbf{h}_{i-1}$, $W_i$ are also independent.
\[
  \begin{aligned}
    Var(\textbf{z}_i [s]) &= \sum_{t = 1}^{D_{i-1}} \mathbb{V} \left[ W_i[s,t] \textbf{h}_{i-1}[t] \right] \\ 
                          &= D_{i-1} \cdot \mathbb{V} \left[W_i[s,t]\textbf{h}_{i-1}[t]\right] \\ 
                          &= D_{i-1} \cdot \left( \mathbb{V}\left[W_i[s,t]\right]\cdot \mathbb{V}\left[\textbf{h}_{i-1}[t]\right] +\mathbb{E}\left[\textbf{h}_{i-1}[t]] \right]^2 \cdot \mathbb{V}\left[ W_i[s,t] \right] + \mathbb{E}\left[ W_i[s,t] \right]^2 \cdot \mathbb{V}\left[ \textbf{h}_{i-1}[t] \right] \right) \\ 
                          &= D_{i-1} \cdot \left( \mathbb{V}\left[W_i[s,t]\right]\cdot \mathbb{V}\left[\textbf{h}_{i-1}[t]\right] +  \mathbb{E}\left[\textbf{h}_{i-1}[t]] \right]^2 \cdot \mathbb{V}\left[ W_i[s,t] \right]\right) \\
                          &= D_{i-1} \cdot \left( \mathbb{V}\left[ W_i[s,t]\right] \cdot \mathbb{E}\left[ \textbf{h}_{i-1}[t]^2\right] \right)
  \end{aligned}
\]
Let's try evaluating $\mathbb{E}\left[ \textbf{h}_{i-1}[t]^2\right]$, let's also denote the pdf of $\textbf{h}_{i-1}[t]$ as some $f_{\textbf{z}_{i-1}}$,
  \[
    \begin{aligned}
      \mathbb{E}\left[ \textbf{h}_{i-1}[t]^2\right] &= \int_{-\infty}^{\infty} (\sigma(\textbf{z}_{i-1}[t]) \cdot f_{\textbf{z}_{i-1}[t]} d\textbf{z}_{i-1}[t] \\ 
                                                    &= \int_0^{\infty} \textbf{z}_{i-1}[t]^2 \cdot f_{\textbf{z}_{i-1}[t]} d\textbf{z}_{i-1}[t] \\ 
                                                    &= \frac{1}{2} \mathbb{V}\left[ \textbf{z}_{i-1}[t] \right]
    \end{aligned}
  \]

  Therefore, we have that,
  \[
    \mathbb{V}[\textbf{z}_i[s]]=D_{i-1} \cdot \mathbb{V}[W_i[s,t]] \cdot \frac{1}{2} \mathbb{V}[\textbf{z}_{i-1}[t]]
  \]
  By unrolling the recursion we get that,
  \[
    \mathbb{V}[\textbf{z}_L[s]] = \mathbb{V}[\textbf{z}_1[t]]  \cdot  \prod_{i=2}^L \frac{D_{i-1}\mathbb{V}[W_i[s,t]]}{2} 
  \]
  And we can simply assume that,
  \[
    \frac{D_{i-1}\mathbb{V}[W_i[s,t]]}{2}=1 \to \mathbb{V}[W_i[s,t]] = \frac{2}{D_{i-1}}
  \]

  \textbf{Backward Pass:} \\ 
  Using the same notation from the previous question, 
  \[
    \begin{aligned}
      \frac{\partial \ell}{\partial \textbf{h}_{i-1}[j]} &= \sum_{k=1}^{D_i} \frac{\partial \ell}{\partial \textbf{z}_i[k]}\frac{\partial \textbf{z}_i[k]}{\partial \textbf{h}_{i-1}[j]} \\ 
                                                         &=  \sum_{k=1}^{D_i} \frac{\partial \ell}{\partial \textbf{z}_i[k]} \sum_m\left(\frac{\partial }{\partial \textbf{h}_{i-1}[j]}\left(W_i[k,m]\textbf{h}_{i-1}[m]\right)\right) \\ 
                                                         &= \sum_{k=1}^{D_i} \frac{\partial \ell}{\partial \textbf{z}_i[k]}W_i[k,j] 
    \end{aligned}
    \]
    Let's assume independence of $W_i[k,j]$ and $\frac{\partial \ell}{\partial \textbf{z}_{i}[k]}$ and we also assume zero mean of $\frac{\partial \ell}{\partial \textbf{h}_{i-1}[k]}$ for all $i$ along with $W_i[k,j]$ being symmetrically distributed around 0.
    
    We can also see that,
    \[
      \frac{\partial \ell}{\partial\textbf{z}_i[k]} = \sigma '\left( \textbf{z}_i[k]\right) \cdot \frac{\partial \ell}{\partial \textbf{h}_i[k]}
  \]
  Here, we assume independence of $\sigma(\textbf{z}_i[k])'$ and $\frac{\partial \ell}{\partial \textbf{h}_i[k]}$.
  We can see that,
  \[
    \begin{aligned}
      \mathbb{E}\left[ \frac{\partial \ell}{\partial \textbf{z}_i[k]} \right] &= \mathbb{E}\left[ \sigma'(\textbf{z}_i[k]) \right] \cdot \mathbb{E}\left[ \frac{\partial \ell}{\partial \textbf{h}_i[k]} \right] \\ 
                                                                              &= 0
    \end{aligned}
    \]
    Then,
    \[
      \begin{aligned}
        \mathbb{V}\left[\frac{\partial \ell}{\partial \textbf{h}_{i-1}[j]}\right] &= \sum_{k=1}^{D_i} \frac{\partial \ell}{\partial \textbf{z}_i[k]}W_i[k,j] \\
                                                                                  &= D_i \cdot \mathbb{V}\left[ \frac{\partial \ell}{\partial \textbf{z}_i[k]}W_i[k,j]  \right] \\ 
                                                                                  &= D_i \cdot \left( \mathbb{V}\left[ W_i[k,j] \right] \cdot \mathbb{E}\left[ \left(\frac{\partial \ell}{\partial \textbf{z}_i[k]} \right)^2 \right] \right) 
      \end{aligned}
    \]

    Let's compute $\mathbb{E}\left[ \left(\frac{\partial \ell}{\partial \textbf{z}_i[k]} \right)^2 \right]$,
    \[
      \begin{aligned}
        \mathbb{E}\left[ \left(\frac{\partial \ell}{\partial \textbf{z}_i[k]} \right)^2\right]&= \mathbb{V} \left[ \frac{\partial \ell}{\partial \textbf{z}_i[k]} \right] \\ 
                                                                                              &= \mathbb{V}\left[ \sigma '\left( \textbf{z}_i[k]\right) \cdot \frac{\partial \ell}{\partial \textbf{h}_i[k]}
 \right] \\
                                                                                              &=\int_{0}^{\infty}  \frac{\partial \ell}{\partial \textbf{h}_i[k]}^2 \cdot f_{\frac{\partial \ell}{\partial \textbf{h}_i[k]}} d\frac{\partial \ell}{\partial \textbf{h}_i[k]} \\ 
                                                                                              &= \frac{1}{2} \cdot \mathbb{V}\left[\frac{\partial \ell}{\partial \textbf{h}_i[k]} \right] 
      \end{aligned}
    \]

    Combining the above expressions we can get,
  \[
    \mathbb{V}\left[\frac{\partial \ell}{\partial \textbf{h}_{i-1}[j]}\right] = D_i \cdot \mathbb{V}\left[ W_i[k,j] \right] \cdot \frac{1}{2} \cdot \mathbb{V}\left[ \frac{\partial \ell}{\partial \textbf{h}_i[k]} \right] 
  \]

  Unrolling the recursion, we can conclude with,
  \[
    \mathbb{V}\left[ \frac{\partial \ell}{\partial \textbf{h}_1[j]}\right] = \mathbb{V}\left[ \frac{\partial \ell}{\partial \textbf{h}_L[k]}\right] \prod_{i=2}^{L} \frac{D_i}{2}\cdot \mathbb{V}[W_i[k,j]]
  \]

  To make it stable, we can again simply assume that,
  \[
    \prod_{i=2}^{L} \frac{D_i}{2}\cdot \mathbb{V}[W_i[k,j]] = 1 \to \mathbb{V}\left[ W_i[k,j] \right] = \frac{2}{D_i}
  \]

  To compromise both goals from forward / backward pass,
  $$\therefore \mathbb{V}[W_i[k,j]] = \frac{1}{\frac{\frac{D_i}{2} + \frac{D_{i-1}}{2}}{2}} = \frac{4}{D_i + D_{i-1}}$$

  \end{solution}
\end{document}
